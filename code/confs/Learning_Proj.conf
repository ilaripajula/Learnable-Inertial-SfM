exp_name = Learning_Proj
#random_seed = 0
dataset
{
    use_gt = False
    calibrated = False
    batch_size = 1
    min_sample_size = 10
    max_sample_size = 20
    test_set = ["Nijo", "Drinking Fountain", "Dino 4983", "Some Cathedral In Barcelona", "Skansen Kronan", "Dome",
        "Dino 319", "Gustav Vasa", "Sri Veeramakaliamman Singapore", "Alcatraz Water Tower"]
    validation_set = ["Thian Hook Keng Temple Singapore", "Pantheon Paris", "Golden Statue"]
    train_set = ["Corridor", "East Indiaman Goteborg", "Sri Thendayuthapani", "Porta San Donato", "Toronto University",
        "De Guerre", "Smolny Cathedral", "Buddah Tooth", "Park Gate", "Alcatraz Courtyard"]
}
model
{
    type = SetOfSet.SetOfSetNet
    num_features = 256
    num_blocks = 1
    block_size = 3
    use_skip = False
    normalize_output = Differentiable Chirality
    multires = 0

}
train
{
    lr = 1e-4
    num_of_epochs = 1e+5
    scheduler_milestone = [50000, 70000, 90000]
    gamma = 0.1
    eval_intervals = 5000
    optimization_num_of_epochs = 500
    optimization_eval_intervals = 250
    optimization_lr = 1e-4
}
ba
{
run_ba = True
repeat=True
triangulation=False  # If repeat, the first time is from our points and the second from triangulation
only_last_eval = True
}
loss
{
    func = ESFMLoss
    infinity_pts_margin = 1e-4
    normalize_grad = True
    hinge_loss = True
    hinge_loss_weight = 1
}


